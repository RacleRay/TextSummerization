{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本摘要的应用场景有很多，比如搜索引擎、观点抽取、新闻、汇报文档等。\n",
    "\n",
    "摘要技术分为两类：\n",
    "\n",
    "- Extractive是从文中找出关键信息，然后拼接进行结果输出\n",
    "    - 关键信息识别抽取\n",
    "- Abstracrtive: 依据文本的输入，生成单词（可能是新的单词）进行结果输出\n",
    "    - Seq2Seq\n",
    "    - Pointer Generator\n",
    "    - Transfomer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "项目流程\n",
    "1. data analysis\n",
    "1. data process\n",
    "1. 基于sentence embedding的关键句信息抽取\n",
    "    - 距离度量：cosine similarity\n",
    "1. 语句流畅性平滑\n",
    "    - 近邻sentence embedding平均化平滑方法\n",
    "1. Title、keywords加权修正\n",
    "    - 标题的embedding赋予更高的权重，在相似性计算时进行处理\n",
    "    - textrank关键词提取，计算sentence embedding时加权处理\n",
    "    - 基于位置信息的加权处理：段落开端，结尾一般会更加重要\n",
    "1. 基于LDA的主题相关性修正"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised extractive Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "raw_data = pd.read_csv('sqlResult_1558435.csv', encoding='gb18030')\n",
    "raw_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "raw_data.feature[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "raw_data.content[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "raw_data.title[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "raw_data.url[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "raw_data.source[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "raw_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.notnull(raw_data.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 筛选掉没有意义的内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "raw_data.content.apply(lambda x: len(str(x))).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "raw_data.content.apply(lambda x: len(str(x))).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "useless_index = []\n",
    "\n",
    "for i, c in raw_data.content.items():\n",
    "    if len(str(c)) <= 120:\n",
    "        useless_index.append(i)\n",
    "        if len(str(c)) > 100:\n",
    "            print(raw_data.url[i], '||', raw_data.content[i])\n",
    "\n",
    "\n",
    "# raw_data.content.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(useless_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "useless_index_long = []\n",
    "\n",
    "for i, c in raw_data.content.items():\n",
    "    if len(str(c)) <= 30000:\n",
    "        useless_index_long.append(i)\n",
    "        if len(str(c)) > 10000:\n",
    "            print(raw_data.url[i], 'Len: ',len(str(c)))\n",
    "            print('index(pandas中):', i)\n",
    "            print(raw_data.content[i][:500])\n",
    "            print('=========')\n",
    "            print(raw_data.content[i][-500:])\n",
    "            print('#########')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "useless_index.extend([3117,6221, 10052, 27862,62823, 48328,62823,76116,79555, 82780, 84244])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "发现：content内容中多次出现\"外代二线\"的新闻没有summarize的需要，处理时应该删除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "useless_index_words = []\n",
    "\n",
    "def deal_some_words():\n",
    "    t = 0\n",
    "    for i, c in raw_data.content.items():\n",
    "        t += 1\n",
    "        split_res = str(c).split('外代二线')\n",
    "        if len(split_res) >= 6:\n",
    "            useless_index_words.append(i)\n",
    "            if t % 5 == 0:\n",
    "                print(raw_data.url[i], 'Len: ',len(str(c)))\n",
    "                print('index(pandas中):', i)\n",
    "                print(raw_data.content[i][:500])\n",
    "                print('=========')\n",
    "                print(raw_data.content[i][-500:])\n",
    "                print('#########')\n",
    "                print()\n",
    "                \n",
    "deal_some_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(useless_index_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in useless_index_words:\n",
    "    if i not in useless_index:\n",
    "        useless_index.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = raw_data.drop(pd.Index(useless_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data.content.apply(lambda x: len(str(x))).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data.to_csv('clean_data_len_gt_120.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "del raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T03:35:01.951721Z",
     "start_time": "2020-05-13T03:35:01.780669Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "#### 分句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('clean_data_len_gt_120.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyltp import SentenceSplitter  # 淘汰\n",
    "\n",
    "\n",
    "def split_sentence(doc):\n",
    "#     doc = doc.strip().replace(u'\\u3000', u'').replace(u'\\\\n', u'。').replace(u'(。)+', u'。').replace(' ', '')\n",
    "    return [sent for sent in SentenceSplitter.split(doc) if len(sent)>5]\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "def split_to_sentence(doc, min_len=6):\n",
    "    \"\"\"自定义的分段分句\n",
    "    \n",
    "    return:\n",
    "        list, 储存内容为每一段分句结果的list，index信息可以用于后续位置特征计算\n",
    "    \"\"\"\n",
    "#     pattern = re.compile(\".*?[。?？!！]\")  # 非贪婪模式匹配文字内容\n",
    "    \n",
    "    paragraph_gen = split_to_paragraph(doc)\n",
    "    doc_content = []\n",
    "    for para in paragraph_gen:\n",
    "        if para is None:\n",
    "            continue\n",
    "        elif len(para) <= min_len:\n",
    "            continue\n",
    "        \n",
    "        doc_content.append(split_sentence(para))\n",
    "#         elif para.strip()[-1] in '。?？!！\"“”':\n",
    "#             sent_of_para = re.findall(pattern, para)\n",
    "#             doc_content.append(sent_of_para)\n",
    "#         else: \n",
    "#             doc_content.append([para])\n",
    "            \n",
    "    return doc_content\n",
    "\n",
    "\n",
    "def split_to_paragraph(doc):\n",
    "    \"\"\"为了识别靠近段开头和结尾位置，需要单独输出句子位置特征\n",
    "    \n",
    "    return:\n",
    "        filter结果生成器\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r\"(\\r\\n\\u3000\\u3000)|(\\r\\n)|(\\u3000\\u3000)|(\\\\n)\")\n",
    "    res = re.split(pattern, doc)\n",
    "    for i in res:\n",
    "        if i and len(i) > 5:\n",
    "            yield i\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "            \n",
    "def split_to_sentence(doc, min_len=6, use_re=False):\n",
    "    \"\"\"自定义的分段分句\n",
    "\n",
    "    return:\n",
    "        list, 储存内容为每一段分句结果的list，index信息可以用于后续位置特征计算\n",
    "    \"\"\"\n",
    "    if use_re:\n",
    "        pattern = re.compile(\".*?[。?？!！]\")  # 非贪婪模式匹配文字内容\n",
    "\n",
    "    paragraph_gen = split_to_paragraph(doc)\n",
    "    doc_content = []\n",
    "    for para in paragraph_gen:\n",
    "        if para is None:\n",
    "            continue\n",
    "        elif len(para) <= min_len:\n",
    "            continue\n",
    "\n",
    "        if not use_re:\n",
    "            doc_content.append(split_sentence(para))\n",
    "        else:\n",
    "            if para.strip()[-1] in '。?？!！\"”':\n",
    "                sent_of_para = re.findall(pattern, para)\n",
    "                doc_content.append(sent_of_para)\n",
    "            else:\n",
    "                doc_content.append([para])\n",
    "\n",
    "    return doc_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "split_to_sentence(data.content[1312])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "from pyhanlp import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "几种主流分词器对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def cut(string): return ' '.join(jieba.cut(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "s = '从大的环境上来看，市场目前本身不具备大面积和大空间的反弹基础，因为目前无论是从宏观面、货币基本面或者从国际经济和政治的角度来看，都不具备这样的条件，所以反应到市场中来，只能是结构性、局部性的投机性机会。而最近半个月以来，市场的走势也确实符合局部性、结构性投机的走势。'\n",
    "cut(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(HanLP.segment(s))\n",
    "\n",
    "print(StandardTokenizer.segment(data.content[1]))\n",
    "\n",
    "StandardTokenizer = JClass('com.hankcs.hanlp.tokenizer.StandardTokenizer')\n",
    "print(StandardTokenizer.segment(s))\n",
    "\n",
    "# 带命名实体识别\n",
    "NLPTokenizer = JClass('com.hankcs.hanlp.tokenizer.NLPTokenizer')\n",
    "print(NLPTokenizer.segment(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "string = '''网易娱乐7月21日报道 林肯公园主唱查斯特·贝宁顿Chester Bennington于今天早上，在洛杉矶帕洛斯弗迪斯的一个私人庄园自缢身亡，年仅41岁。此消息已得到洛杉矶警方证实。\n",
    "　　洛杉矶警方透露，Chester的家人正在外地度假，Chester独自在家，上吊地点是家里的二楼。一说是一名音乐公司工作人员来家里找他时发现了尸体，也有人称是佣人最早发现其死亡。\n",
    "　　林肯公园另一位主唱麦克·信田确认了Chester Bennington自杀属实，并对此感到震惊和心痛，称稍后官方会发布声明。Chester昨天还在推特上转发了一条关于曼哈顿垃圾山的新闻。粉丝们纷纷在该推文下留言，不相信Chester已经走了。\n",
    "　　外媒猜测，Chester选择在7月20日自杀的原因跟他极其要好的朋友、Soundgarden(声音花园)乐队以及Audioslave乐队主唱Chris Cornell有关，因为7月20日是Chris Cornell的诞辰。而Chris Cornell于今年5月17日上吊自杀，享年52岁。Chris去世后，Chester还为他写下悼文。\n",
    "　　对于Chester的自杀，亲友表示震惊但不意外，因为Chester曾经透露过想自杀的念头，他曾表示自己童年时被虐待，导致他医生无法走出阴影，也导致他长期酗酒和嗑药来疗伤。目前，洛杉矶警方仍在调查Chester的死因。\n",
    "　　据悉，Chester与毒品和酒精斗争多年，年幼时期曾被成年男子性侵，导致常有轻生念头。Chester生前有过2段婚姻，育有6个孩子。\n",
    "　　林肯公园在今年五月发行了新专辑《多一丝曙光One More Light》，成为他们第五张登顶Billboard排行榜的专辑。而昨晚刚刚发布新单《Talking To Myself》MV。'''\n",
    "\n",
    "print(HanLP.extractSummary(string, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm, tqdm_notebook\n",
    "import itertools\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "StandardTokenizer = JClass('com.hankcs.hanlp.tokenizer.StandardTokenizer')\n",
    "\n",
    "def segment(content):\n",
    "    \"\"\"在split_to_sentence的基础上，生成分词文件。采用hanlp的StandardTokenizer。\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    total_tokens = []\n",
    "    sents = split_to_sentence(content)\n",
    "    for sent in chain.from_iterable(sents):\n",
    "        tokens = [item.word for item in StandardTokenizer.segment(sent)]\n",
    "        total_tokens.extend(tokens)\n",
    "    return ' '.join(total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test = data[: 20].copy()\n",
    "test.content[2]\n",
    "test.content.apply(segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test.tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data['tokens'] = data.content.progress_apply(segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data.tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data.to_csv('clean_data_len_gt_120.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('tokens.txt', 'w', encoding='utf-8') as f:\n",
    "    for c in tqdm_notebook(data.tokens):\n",
    "        f.write(c)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "corpus_file = 'tokens.txt'  # absolute path to corpus\n",
    "model = FastText(window=5, size=200,  min_count=1, workers=2)\n",
    "model.build_vocab(corpus_file=)  # scan over corpus to build the vocabulary\n",
    "\n",
    "total_words = model.corpus_total_words  # number of words in the corpus\n",
    "model.train(corpus_file=corpus_file, total_words=total_words, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 预料太少，无监督模式的fasttext效果依然不好\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "%%time\n",
    "word_vec = KeyedVectors.load_word2vec_format('sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300.iter5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word_vec.similar_by_word('伤心')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 词频、概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "def count_gen():\n",
    "    corpus_dict = []\n",
    "    for c in tqdm_notebook(data.tokens):\n",
    "        corpus_dict.extend(c.split())\n",
    "    return corpus_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dict = count_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "total_counter = Counter(corpus_dict)\n",
    "\n",
    "length = len(corpus_dict)\n",
    "\n",
    "frequence = {w: c/length for w, c in total_counter.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('frequence.bin', 'wb') as f:\n",
    "    pickle.dump(frequence, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('frequence.bin', 'rb') as f:\n",
    "    frequence = pickle.load(f, encoding='uft-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurences_frequences = sorted(list(frequence.values()), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurences_frequences[: 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurences_frequences[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Textrank关键词、关键句抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = JClass('com.hankcs.hanlp.tokenizer.StandardTokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_list = [\"除小米手机6等15款机型外，其余机型已暂停更新发布，以确保工程师可以集中全部精力进行系统优化工作。\", \"有人猜测这也是将精力主要用到MIUI 9的研发之中。\", \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于生成测试数据\n",
    "sent_word_list = []\n",
    "for sent in sent_list[:-1]:\n",
    "    tmp = []\n",
    "    for item in tokenizer.segment(sent):\n",
    "        if str(item.nature) in allow_speech_tags and str(item.word) not in stopwords:\n",
    "            tmp.append(item.word)\n",
    "    sent_word_list.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwordslist():\n",
    "    \"\"\"创建停用词列表\"\"\"\n",
    "    stopwords = {line.strip()\n",
    "        for line in open('stopwords.txt', encoding='UTF-8').readlines()}\n",
    "    stopwords.add('\\u3000')\n",
    "    return stopwords\n",
    "    \n",
    "stopwords = stopwordslist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import chain\n",
    "from pyhanlp import *\n",
    "from math import log\n",
    "# from utils import split_to_sentence\n",
    "\n",
    "class TextRank:\n",
    "    def __init__(self, stopwords, allowed_pos):\n",
    "        self.stopwords = stopwords\n",
    "        self.allowed_pos = frozenset(allowed_pos)\n",
    "\n",
    "        self.tokenizer = JClass('com.hankcs.hanlp.tokenizer.StandardTokenizer')\n",
    "\n",
    "    def process_input(self, doc_str, case='keyword'):\n",
    "        \"处理输入文档。输出结果格式为：[['sent', 'one', 'words'],['sent', 'two', 'words']]\"\n",
    "        self.sent_of_words = []\n",
    "        sent_list = split_to_sentence(doc_str)\n",
    "        for sent in chain.from_iterable(sent_list):\n",
    "            tmp = []\n",
    "            for item in self.tokenizer.segment(sent):\n",
    "                if case == 'keyword':\n",
    "                    tmp.append(item)\n",
    "                elif case == 'keysentence':\n",
    "                    tmp.append(str(item.word))\n",
    "            self.sent_of_words.append(tmp)\n",
    "\n",
    "    def get_keywords(self, doc_str, num=10, min_len=2, span=5):\n",
    "        self.process_input(doc_str, case='keyword')\n",
    "        # 找到候选\n",
    "        count_dic = defaultdict(int)\n",
    "        word_set = set()\n",
    "        for sent in self.sent_of_words:\n",
    "            for i, w_item in enumerate(sent):\n",
    "                if self.filte_words(w_item, min_len):\n",
    "                    word = str(w_item.word)\n",
    "                    word_set.add(word)\n",
    "                    for j in range(i+1, i+span):\n",
    "                        if j >= len(sent): break\n",
    "                        if not self.filte_words(sent[j], min_len):\n",
    "                            continue\n",
    "                        word_j = str(sent[j].word)\n",
    "                        word_set.add(word_j)\n",
    "                        count_dic[(word, word_j)] += 1\n",
    "                        count_dic[(word_j, word)] += 1\n",
    "        word2id = {word: i for i, word in enumerate(list(word_set))}\n",
    "        id2word = {i: word for word, i in word2id.items()}\n",
    "\n",
    "        # 建立共线矩阵\n",
    "        num_of_nodes = len(word2id)\n",
    "        weight_M = np.zeros((num_of_nodes, num_of_nodes))\n",
    "        for (wi, wj), weight in count_dic.items():\n",
    "            i = word2id[wi]\n",
    "            j = word2id[wj]\n",
    "            weight_M[i, j] = weight\n",
    "        \n",
    "        weight_M = np.nan_to_num(weight_M / np.linalg.norm(weight_M, ord=1, axis=0, keepdims=True))\n",
    "        # pagerank求解\n",
    "        textrank_v = self.page_rank(weight_M)\n",
    "        result = sorted([(id2word[i], value) for i, value in enumerate(textrank_v)],\n",
    "                        key=lambda x: x[1],\n",
    "                        reverse=True)\n",
    "        return result[: num]\n",
    "\n",
    "    def get_keysentences(self, doc_str, num=6, min_len=5):\n",
    "        \"\"\"由于sentence在一段话中几乎不可能出现完全一样的情况，因此只基于共现的pagerank是行不通的。\n",
    "        引入BM25，来计算句子与句子之间的关联权重。注：BM25原本是用来计算query句子和文档之间的相似度，用于信息检索的、\n",
    "        \"\"\"\n",
    "        self.process_input(doc_str, case='keysentence')\n",
    "        total_sent = len(self.sent_of_words)\n",
    "\n",
    "        weight_M = np.zeros((total_sent, total_sent))\n",
    "        for i in range(total_sent):\n",
    "            sent_i = self.sent_of_words[i]\n",
    "            for j in range(total_sent):\n",
    "                if i == j: continue\n",
    "                sent_j = self.sent_of_words[j]\n",
    "                # 权重矩阵中的i行，j列\n",
    "                weight_M[i, j] = self.sent_corelation_func(sent_i, sent_j)\n",
    "        \n",
    "        weight_M = np.nan_to_num(weight_M / np.linalg.norm(weight_M, ord=1, axis=0, keepdims=True))\n",
    "        \n",
    "        sent_para = split_to_sentence(doc_str)\n",
    "        ps_weight = get_position_weight(sent_para)\n",
    "        textrank_v = self.page_rank(weight_M) * np.array(ps_weight)\n",
    "        print(textrank_v)\n",
    "        result_id = sorted([(idx, value) for idx, value in enumerate(textrank_v)],\n",
    "                           key=lambda x: x[1],\n",
    "                           reverse=True\n",
    "                           )\n",
    "        count = 0\n",
    "        result_sent = []\n",
    "        for (i, value) in result_id:\n",
    "            if count >= num:\n",
    "                break\n",
    "            sent = ''.join(self.sent_of_words[i])\n",
    "            if len(sent) <= min_len:\n",
    "                continue\n",
    "            result_sent.append((sent, value, i))\n",
    "            count += 1\n",
    "        \n",
    "        result_sent = sorted(result_sent, key=lambda x: x[2])\n",
    "        return result_sent\n",
    "\n",
    "\n",
    "    def get_tf(self, sent_i, sent_j):\n",
    "        \"\"\"计算bm25的term frequence. sent来自预处理的sent_of_words列表。\"\"\"\n",
    "        freq = {}\n",
    "        sent_i_counts = Counter(sent_i)\n",
    "        # 计算i句中的词，在j句中的tf\n",
    "        for w in sent_j:\n",
    "            # if not self.filte_words(w_item):\n",
    "            #     continue\n",
    "            if w in sent_i_counts:\n",
    "                freq[w] = sent_i_counts[w]\n",
    "            else:\n",
    "                freq[w] = 0\n",
    "        total = len(sent_i)\n",
    "        return {word: count / total for word, count in freq.items()}\n",
    "\n",
    "    def get_idf(self):\n",
    "        \"\"\"计算inverse document frequence. 这里计算句子的相似度，所以计算inverse sentence frequence\"\"\"\n",
    "        total_sent = len(self.sent_of_words) + 1 # 假设有一个句子包含所有词\n",
    "        avg_len = 0\n",
    "        doc_freq = {}\n",
    "        for sent in self.sent_of_words:\n",
    "            avg_len += len(sent)\n",
    "            words = list({w for w in sent})\n",
    "            for word in words:\n",
    "                # 假设有一个句子包含所有词\n",
    "                doc_freq[word] = doc_freq.setdefault(word, 0) + 1\n",
    "        avg_len /= total_sent\n",
    "        # sklearn实现\n",
    "        idf = {word: log(total_sent / df) + 1 for word, df in doc_freq.items()}\n",
    "        return idf, avg_len\n",
    "\n",
    "    def filte_words(self, w_item, min_len=2):\n",
    "        word = str(w_item.word)\n",
    "        pos = str(w_item.nature)\n",
    "        return (pos in self.allowed_pos and word not in self.stopwords\n",
    "                and len(word) >= min_len)\n",
    "\n",
    "    def sent_corelation_func(self, sent_i, sent_j, k1=1.5, b=0.75):\n",
    "        \"\"\"计算bm25。\n",
    "\n",
    "        sent_i ： 与query对比的句子，在文档中进行遍历\n",
    "        sent_j : query的句子\n",
    "        \"\"\"\n",
    "        idf, avg_len = self.get_idf()\n",
    "        tf = self.get_tf(sent_i, sent_j)\n",
    "\n",
    "        K = k1 * (1 - b + b * len(sent_i) / avg_len)\n",
    "        bm25 = 0\n",
    "        for j_word in sent_j:\n",
    "            bm25 += idf[j_word] * tf[j_word] * (k1 + 1) / (tf[j_word] + K)\n",
    "        return bm25\n",
    "\n",
    "    @staticmethod\n",
    "    def page_rank(weight_M, iterations=100, d=0.85):\n",
    "        \"\"\"\n",
    "        weight_M: 对于textRank，这是窗口遍历文档所得的符合条件的边的权重矩阵。\n",
    "                  pageRank中第i行、第j列表示：从j节点到i节点的链接权重。\n",
    "                  但是textRank是无向图，只是两者的共性关系权重。\n",
    "        d： 衰减系数，防止局部陷入无法向外链接\n",
    "        \"\"\"\n",
    "        N = weight_M.shape[1]\n",
    "        v = np.random.rand(N, 1)\n",
    "        v = v / np.linalg.norm(v, 1)\n",
    "        M_hat = (d * weight_M + (1 - d) / N)\n",
    "        for i in range(iterations):\n",
    "            v = M_hat @ v\n",
    "        return v.ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allow_speech_tags = set('nz ni ntc j ntcb nt nhm nic nn t g n nnd ntch nit gb gbc nb nnt nba nr an gc nbc nr1 gg nbp nr2 gi nf nrf gm ng nrj gp nh ns nhd nsf i v vl vi vd nl'.split())\n",
    "\n",
    "# jieba\n",
    "# allow_speech_tags =  ['an', 'i', 'j', 'l', 'n', 'nr', 'nrfg', 'ns', 'nt', 'nz', 't', 'v', 'vd', 'vn', 'eng']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textrank = TextRank(stopwords, allow_speech_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.content[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textrank.get_keywords(data.content[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textrank.get_keysentences(data.content[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(textrank.get_keywords(data.content[11]), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in HanLP.extractKeyword(data.content[11], 10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string ='''据国外媒体报道，据英国《卫报》报道，热带飓风“阿加莎”31日席卷中美洲，它带来的倾盆大雨已经夺去100人的性命。在这场飓风的影响下，危地马拉首都危地马拉城出现一个深达60米的塌陷洞，据说有一栋3层建筑坠入洞中。    受2010年首个太平洋热带风暴影响，危地马拉城积聚了1米多深的雨水，这场风暴还影响到萨尔瓦多和洪都拉斯。据报告上称，目前危地马拉至少已有113人丧生，大约有50人失踪，营救队员正在一片瓦砾中进行搜救。    这个直径30米的塌陷洞位于危地马拉北部地区。当地居民称，雨水和排水系统不完善导致地面塌陷。当地报道表示，在那座3层建筑物坠入地洞时，至少有1人丧生。2007年，这一地区出现类似塌陷洞，当时有3人丧生。    危地马拉是受“阿加莎”影响最严重的一个国家，经证实，该地目前死亡人数已达92人，搜救人员进入偏远农村后，这一数字有可能还会继续上升。有大约10万人被迫撤离家园。警方称，萨尔瓦多有9人死亡，洪都拉斯有12人丧生。    阿马蒂特兰的卡尔洛塔·拉莫斯站在几乎被淤泥淹没的房屋前悲伤地说：“没有人可以帮助我。我眼睁睁看着雨水冲走了一切。'''\n",
    "\n",
    "from jieba import analyse\n",
    "\n",
    "textrank = analyse.textrank\n",
    "\n",
    "text = string\n",
    "\n",
    "allowPOS = ('an', 'i', 'j', 'l', 'n', 'nr', 'nrfg', 'ns', 'nt', 'nz', 't', 'v', 'vd', 'vn', 'eng')\n",
    "\n",
    "print(\"keywords by textrank:\")\n",
    "keywords = textrank(\n",
    "    text,\n",
    "    topK=10,\n",
    "    withWeight=True,\n",
    "    allowPOS=allowPOS,\n",
    "    withFlag=False)\n",
    "\n",
    "words = [(keyword, w) for keyword, w in keywords if w > 0.1]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sentence embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from pyhanlp import *\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec = KeyedVectors.load_word2vec_format('sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300.iter5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = JClass('com.hankcs.hanlp.tokenizer.StandardTokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_sentences_vec_mat(sent_list, prob_dict, param_a=0.0001):\n",
    "    \"\"\"计算sentence vector，在原论文的基础上进行修改，语义建模引入整个文档和标题信息。\n",
    "    来自paper:\n",
    "    A SIMPLE  BUTTOUGH-TO-BEATBASELINE  FORSEN-TENCEEMBEDDINGS. ICLR2017\n",
    "    \n",
    "    sent_list: 来自待识别文档的分句结果, list；\n",
    "    prob_dict: 语料库中的token概率值, dict；\n",
    "    param_a: 论文中实验得到的效果比较好的参数取值, 1e-3 ~ 1e-5；\n",
    "    \n",
    "    return: \n",
    "        matrix--(vector_dim, sentence_num + 1) \n",
    "                形状的matrix，每一列代表sentence的向量. 多出的1为doc的向量.\n",
    "                sentence_num中最后一个sent为title\n",
    "        doc_vector--整个文档的向量表达\n",
    "    \"\"\"\n",
    "    row_size = word_vec.vector_size\n",
    "    col_size = len(sent_list)\n",
    "    \n",
    "    doc_vector = np.zeros(row_size)\n",
    "    matrix = np.zeros((row_size, col_size + 1))  # +1为整个文档的向量表示\n",
    "    \n",
    "    default_p = max(prob_dict.values())\n",
    "    doc_len = 0\n",
    "    for i, sentence in enumerate(sent_list):\n",
    "        sentence = tokenizer.segment(sentence)\n",
    "        sent_len = len(sentence)\n",
    "        doc_len += 1\n",
    "        \n",
    "        sent_vector = matrix[:, i]\n",
    "        for item in sentence:  # 计算第i句的sent_vector\n",
    "            token = str(item.word)\n",
    "            pw = prob_dict.setdefault(token, default_p)\n",
    "            weight = param_a / (param_a + pw)\n",
    "            try:\n",
    "                word_vector = np.array(word_vec.get_vector(token))\n",
    "                sent_vector += weight * word_vector\n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        matrix[:, i] = sent_vector / sent_len\n",
    "        doc_vector += matrix[:, i]\n",
    "    matrix[:, -1] = doc_vector / doc_len\n",
    "    \n",
    "    print(matrix)\n",
    "    matrix = np.nan_to_num(matrix)\n",
    "    # PCA找到整个矩阵中，每个句子中最相似的部分（第一个主成分），然后减去相似部分\n",
    "    U, s, Vh = np.linalg.svd(matrix)  # 默认s降序\n",
    "    u = U[:, 0]  # 第一个主成分\n",
    "    matrix -= np.outer(u, u.T) @ matrix  # 每个sent_vector减去在第一个主成分方向的投影\n",
    "    \n",
    "    doc_vector = matrix[:, -1]\n",
    "    title_vector = matrix[:, -2]\n",
    "    return matrix, title_vector, doc_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.array([1,2,3]).T\n",
    "matrix = np.zeros((3, 3))\n",
    "matrix[:, -1] = np.array([1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.outer(u, u.T) @ matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position_weight(sent_para):\n",
    "    \"\"\"开头，结尾增加一些权重\n",
    "    \n",
    "    return:\n",
    "        从第一句到最后一句的位置权重，list\n",
    "    \"\"\"\n",
    "    pos_sent_weight = []\n",
    "    first_para_flag = True\n",
    "    \n",
    "    for i, para in enumerate(sent_para):\n",
    "        if len(para) > 1:\n",
    "            # 每一段开头结尾\n",
    "            tmp = [1.1] + [1. for i in range(len(para)-2)] + [1.08] \n",
    "        else:\n",
    "            tmp = [1.]\n",
    "        \n",
    "        # 第一段\n",
    "        if first_para_flag:\n",
    "            tmp = [1.1 * i for i in tmp]\n",
    "            first_para_flag = False\n",
    "        # 最后一段\n",
    "        elif i == len(sent_para) - 1 and len(para[-1]) >= 10:\n",
    "            tmp = [1.08 * i for i in tmp]\n",
    "        \n",
    "        pos_sent_weight.extend(tmp)\n",
    "    return pos_sent_weight\n",
    "\n",
    "\n",
    "def neighbor_smooth():\n",
    "    \"\"\".\"\"\"\n",
    "    # 在计算embedding时，计入\n",
    "    pass\n",
    "\n",
    "\n",
    "# def get_title_info(title, prob_dict, param_a=0.0001):\n",
    "#     \"\"\"标题信息\"\"\"\n",
    "#     tokens = [item.word for item in StandardTokenizer.segment(title)]\n",
    "#     size = word_vec.vector_size\n",
    "    \n",
    "#     default_p = max(prob_dict.values())\n",
    "#     title_vec = np.zeros(size)\n",
    "#     print(tokens)\n",
    "#     for word in tokens:\n",
    "#         pw = prob_dict.setdefault(word, default_p)\n",
    "#         weight = param_a / (param_a + pw)\n",
    "#         try:\n",
    "#             word_vector = np.array(word_vec.get_vector(word))\n",
    "#             title_vec += weight * word_vector\n",
    "#         except Exception:\n",
    "#             continue\n",
    "#     title_vec /= len(tokens)\n",
    "#     return title_vec, tokens\n",
    "\n",
    "\n",
    "def get_keywords(content):\n",
    "    \"\"\"对包含的关键字/词句子增加其权重\"\"\"\n",
    "    # textrank获取关键词\n",
    "    # 加权再get_position_weight中实现\n",
    "    return HanLP.extractKeyword(content, 5)\n",
    "\n",
    "\n",
    "\n",
    "from gensim import corpora, models, similarities\n",
    "    \n",
    "lda = models.LdaModel.load('lda_model.bin')\n",
    "dictionary = corpora.Dictionary.load('lda_dictionary.bin')\n",
    "\n",
    "num_topics = 10\n",
    "topic_words_dist = []\n",
    "for topicid in range(num_topics):\n",
    "    topic_words = [w for w, _ in lda.show_topic(topicid, topn=10)]\n",
    "    topic_words_dist.append(topic_words)\n",
    "\n",
    "\n",
    "def get_topic_distribution(sent_para):\n",
    "    \"\"\"用每句话和的出来的这些主题进行相似度对比，我们不仅仅是是考虑他的整个的text， 我们还有考虑主题.\n",
    "    使用LDA主题模型，得到的主题分布。\n",
    "    \n",
    "    return:\n",
    "        topic_dist：\n",
    "            format--[(1, 0.018213129),\n",
    "                    (2, 0.06460305),\n",
    "                    (3, 0.114253126),\n",
    "                    (5, 0.21796304),\n",
    "                    (6, 0.03961128),\n",
    "                    (9, 0.5442903)]\n",
    "    \"\"\"\n",
    "    tokens = segment(sent_para, stopwords)\n",
    "    bow_doc = dictionary.doc2bow(tokens)\n",
    "    topic_dist = lda.get_document_topics(bow_doc)\n",
    "    \n",
    "    # 根据主题分布，和每个主题中word的分布，获得需要的主题词的分布\n",
    "    return topic_dist\n",
    "\n",
    "\n",
    "def cal_topic_embedding(sent_para, prob_dict, param_a=0.0001):\n",
    "    \"\"\"根据主题分布，每个主题的词分布，获取topic embedding。\n",
    "    \n",
    "    return:\n",
    "        vector_size大小的一维vector\n",
    "    \"\"\"\n",
    "    topic_dist = get_topic_distribution(sent_para)\n",
    "    size = word_vec.vector_size\n",
    "    \n",
    "    default_p = max(prob_dict.values())\n",
    "    topics_vector = np.zeros(size)\n",
    "    while topic_dist:\n",
    "        # topic weight加权\n",
    "        topicid, t_weight = topic_dist.pop()\n",
    "        topic_words = topic_words_dist[topicid]\n",
    "        \n",
    "        # 与计算sentence embedding的方法保持一致\n",
    "        topic_vector = np.zeros(size)\n",
    "        for word in topic_words:\n",
    "            pw = prob_dict.setdefault(word, default_p)\n",
    "            w_weight = param_a / (param_a + pw)\n",
    "            try:\n",
    "                word_vector = np.array(word_vec.get_vector(word))\n",
    "                topic_vector += w_weight * word_vector\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        topics_vector += topic_vector\n",
    "        \n",
    "    topics_vector /= num_topics * 10  # 每个topic选取10个词来表示\n",
    "        \n",
    "    return topics_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(doc, title, window):\n",
    "    sent_para = split_to_sentence(doc)\n",
    "    pos_sent_weight = get_position_weight(sent_para)\n",
    "    \n",
    "    sent_list = [sent.strip() for sent in chain.from_iterable(sent_para)]\n",
    "\n",
    "    if not title: title = sent_list[0] + sent_list[1]\n",
    "    sent_list.append(title)\n",
    "    \n",
    "#     print(sent_list)\n",
    "    sent_vecs, title_vec, doc_vec = cal_sentences_vec_mat(sent_list, frequence)\n",
    "#     print(sent_vecs)\n",
    "#     print()\n",
    "#     print(doc_vec)\n",
    "#     print()\n",
    "    \n",
    "    # 由于lda从文档中抽象出topic实际上时对语义信息的另一种建模，不加入sentence embedding算法实现\n",
    "    topics_vec = cal_topic_embedding(sent_para, frequence)\n",
    "#     print(topics_vec)\n",
    "#     print()\n",
    "    \n",
    "#     title_vec, tokens = get_title_info(title, frequence)\n",
    "#     print(title_vec)\n",
    "\n",
    "#     keyword\n",
    "    textrank = TextRank(stopwords, allow_speech_tags)\n",
    "    keywords = textrank.get_keywords(doc)\n",
    "    \n",
    "    keysentence = textrank.get_keysentences(doc)\n",
    "    \n",
    "    print(keywords)\n",
    "    \n",
    "    scores = []\n",
    "    print(len(sent_list))\n",
    "    print(sent_vecs.shape)\n",
    "    for i in range(sent_vecs.shape[1] - 2):\n",
    "        sent_to_doc = cosine(sent_vecs[:, i], doc_vec) * pos_sent_weight[i]\n",
    "        sent_to_topic = cosine(sent_vecs[:, i], topics_vec)\n",
    "        sent_to_title = cosine(sent_vecs[:, i], title_vec)\n",
    "        \n",
    "        score = sent_to_doc\n",
    "        score = sent_to_doc + sent_to_topic + sent_to_title\n",
    "        \n",
    "        for i, (kw, values) in enumerate(keywords):\n",
    "            if kw in sent_list[i]:\n",
    "                # 根据value大小顺序，递减权重\n",
    "                score *= (1 + 0.5 * (10 - i * 0.5) / 10)\n",
    "\n",
    "        scores.append(score)\n",
    "    \n",
    "#     print(keywords)\n",
    "    \n",
    "    # 对于一个sentence，它的重要性，取决于本身的重要性和周围的句子(neighbors)的重要性的综合\n",
    "    for i in range(window):\n",
    "        scores.insert(0, scores[0])\n",
    "        scores.append(scores[-1])\n",
    "    weight = np.array([0.25, 0.5, 0.25])\n",
    "    print(scores)\n",
    "    scores = np.array(scores)\n",
    "    score_smooth = [np.dot(scores[i - window: i + window + 1], weight) for i in range(window, len(sent_list) - 1 + window)]\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    assert len(sent_list) - 1 == len(score_smooth)\n",
    "    print(pos_sent_weight)\n",
    "    print(score_smooth)\n",
    "    sorted_idx = np.argsort(score_smooth)[-len(sent_list)//3: ]\n",
    "    sent_ids = sorted(sorted_idx)\n",
    "    for i in sent_ids:\n",
    "        print(sent_list[i])\n",
    "    \n",
    "    print(''.join([sent_list[i] for i in sent_ids]))\n",
    "    print(keysentence)\n",
    "    \n",
    "# get_summary(data.content[1312], data.title[1312], window=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string ='''据国外媒体报道，据英国《卫报》报道，热带飓风“阿加莎”31日席卷中美洲，它带来的倾盆大雨已经夺去100人的性命。在这场飓风的影响下，危地马拉首都危地马拉城出现一个深达60米的塌陷洞，据说有一栋3层建筑坠入洞中。    受2010年首个太平洋热带风暴影响，危地马拉城积聚了1米多深的雨水，这场风暴还影响到萨尔瓦多和洪都拉斯。据报告上称，目前危地马拉至少已有113人丧生，大约有50人失踪，营救队员正在一片瓦砾中进行搜救。    这个直径30米的塌陷洞位于危地马拉北部地区。当地居民称，雨水和排水系统不完善导致地面塌陷。当地报道表示，在那座3层建筑物坠入地洞时，至少有1人丧生。2007年，这一地区出现类似塌陷洞，当时有3人丧生。    危地马拉是受“阿加莎”影响最严重的一个国家，经证实，该地目前死亡人数已达92人，搜救人员进入偏远农村后，这一数字有可能还会继续上升。有大约10万人被迫撤离家园。警方称，萨尔瓦多有9人死亡，洪都拉斯有12人丧生。    阿马蒂特兰的卡尔洛塔·拉莫斯站在几乎被淤泥淹没的房屋前悲伤地说：“没有人可以帮助我。我眼睁睁看着雨水冲走了一切。'''\n",
    "title = '''危地马拉受热带风暴影响出现60米深巨大陷坑'''\n",
    "get_summary(string, title, window=1)b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\"网易娱乐7月21日报道 林肯公园主唱查斯特·贝宁顿Chester Bennington于今天早上，在洛杉矶帕洛斯弗迪斯的一个私人庄园自缢身亡，年仅41岁。此消息已得到洛杉矶警方证实。\n",
    "\n",
    "　　洛杉矶警方透露，Chester的家人正在外地度假，Chester独自在家，上吊地点是家里的二楼。一说是一名音乐公司工作人员来家里找他时发现了尸体，也有人称是佣人最早发现其死亡。\n",
    "\n",
    "　　林肯公园另一位主唱麦克·信田确认了Chester Bennington自杀属实，并对此感到震惊和心痛，称稍后官方会发布声明。Chester昨天还在推特上转发了一条关于曼哈顿垃圾山的新闻。粉丝们纷纷在该推文下留言，不相信Chester已经走了。\n",
    "　　外媒猜测，Chester选择在7月20日自杀的原因跟他极其要好的朋友、Soundgarden(声音花园)乐队以及Audioslave乐队主唱Chris Cornell有关，因为7月20日是Chris Cornell的诞辰。而Chris Cornell于今年5月17日上吊自杀，享年52岁。Chris去世后，Chester还为他写下悼文。\n",
    "　　对于Chester的自杀，亲友表示震惊但不意外，因为Chester曾经透露过想自杀的念头，他曾表示自己童年时被虐待，导致他医生无法走出阴影，也导致他长期酗酒和嗑药来疗伤。目前，洛杉矶警方仍在调查Chester的死因。\n",
    "　　据悉，Chester与毒品和酒精斗争多年，年幼时期曾被成年男子性侵，导致常有轻生念头。Chester生前有过2段婚姻，育有6个孩子。\n",
    "　　林肯公园在今年五月发行了新专辑《多一丝曙光One More Light》，成为他们第五张登顶Billboard排行榜的专辑。而昨晚刚刚发布新单《Talking To Myself》MV。\"\"\"\n",
    "t = \"\"\"林肯公园主唱查斯特·贝宁顿自缢身亡，年仅41岁\"\"\"\n",
    "get_summary(s, t, window=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_summary(data.content[13], data.title[13], window=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lda train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "StandardTokenizer = JClass('com.hankcs.hanlp.tokenizer.StandardTokenizer')\n",
    "\n",
    "def segment(sentences, stopwords):\n",
    "    \"\"\"在split_to_sentence的基础上，生成分词文件。采用hanlp的StandardTokenizer。\n",
    "    \n",
    "    return:\n",
    "        list of tokens for a doc.\n",
    "    \"\"\"\n",
    "    total_tokens = []\n",
    "    for sent in chain.from_iterable(sentences):\n",
    "        tokens = [item.word for item in StandardTokenizer.segment(sent) \\\n",
    "                  if item.word not in stopwords]\n",
    "        total_tokens.extend(tokens)\n",
    "    return total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train lda\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "# input format: [['this', 'is', 'doc', 'one']，\n",
    "#                ['this', 'is', 'doc', 'two']]\n",
    "\n",
    "# 计算sentence embedding时，考虑要不要删除stopwords\n",
    "# 由于计算sentence embedding的输入和lda的输入不一样，因此需要单独处理\n",
    "# textrank的输入是和lda类似的。\n",
    "\n",
    "to_lda = []\n",
    "for doc in data.tokens:\n",
    "    tokens = [token for token in doc.split(' ') if token not in stopwords]\n",
    "    to_lda.append(tokens)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping between normalized words and their integer ids.\n",
    "dictionary = corpora.Dictionary(to_lda)\n",
    "\n",
    "# bag of words\n",
    "corpus = [dictionary.doc2bow(text) for text in to_lda]\n",
    "# LDA模型（can be updated (trained) with new documents.）\n",
    "# 参考cctv新闻网的新闻种类划分，topic选择12类\n",
    "lda = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, alpha='auto')\n",
    "\n",
    "dictionary.save('lda_dictionary.bin')\n",
    "\n",
    "lda.save('lda_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens = segment(data.content[12], stopwords)\n",
    "test_bow = dictionary.doc2bow(test_tokens)\n",
    "\n",
    "lda.get_document_topics(test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([i[1] for i in lda[test_bow]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new corpus, made of previously unseen documents.\n",
    ">>> other_texts = [\n",
    "...     ['computer', 'time', 'graph'],\n",
    "...     ['survey', 'response', 'eps'],\n",
    "...     ['human', 'system', 'computer']\n",
    "... ]\n",
    ">>> other_corpus = [common_dictionary.doc2bow(text) for text in other_texts]\n",
    ">>>\n",
    ">>> unseen_doc = other_corpus[0]\n",
    ">>> vector = lda[unseen_doc]  # get topic probability distribution for a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the model by incrementally training on the new corpus\n",
    ">>> lda.update(other_corpus)\n",
    ">>> vector = lda[unseen_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.show_topics(num_topics=10, num_words=15, log=False, formatted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.show_topic(1, topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 旅游相关\n",
    "lda.show_topic(1, topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 经济\n",
    "lda.show_topic(2, topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 教育\n",
    "lda.show_topic(3, topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 国际贸易\n",
    "lda.show_topic(4, topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 政治\n",
    "lda.show_topic(5, topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文化\n",
    "lda.show_topic(6, topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 国际安全\n",
    "lda.show_topic(7, topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 政府政策\n",
    "lda.show_topic(8, topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 体育\n",
    "lda.show_topic(9, topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重复了\n",
    "lda.show_topic(10, topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重复了\n",
    "lda.show_topic(11, topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda.print_topics(num_topics=20, num_words=10)\n",
    "# lda.print_topic(topicno, topn=10)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
